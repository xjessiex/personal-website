---
title:  "Sentimental Analysis: Watt do you think of your EV?"
date:   2018-12-04
tags: [R, Website Scrapping]
categories: [Projects]
excerpt: "A text mining practice on the reviews of Nissan Leaf. From this content analysis project, I found out that while EV customers have given consistant ratings over time, they have been using more positive words in their reviews."
layout: single
header:
    image: /images/posts/evscrape/header.png
author_profile: true
comments: true
---
Inspired by a paper on content analysis of auto reviews, I was eager to do another side project to apply sentiment analysis on auto reviews. Sentiment analysis of auto reviews enables us to analyze contextual polarity or emotional reactions of the customers. Since I recently spotted a Nissan LEAF in the parking garage at school, I decided to scrape all the reviews (around 1350) for Nissan LEAF model 2015, 2016 and 2017. I collected the dates of the comments, reviewers' names, purchased car models, and the comments. I will be adding some snippets of code into this post so skip them if you want. The plots won't disappoint.

OH! Another goal of this sentiment analysis project is to try something new on a time budget. After talking to several mentors I met on LinkedIn, I realized that doing research can be time-restricting outside of school. When the funding and resources are limited, I need to learn how to compromise on certain aspects of the research and prioritize my goals. So I started off this project by allocating one day to four stages of the project: scraping, sentimental analysis, editing results, and writing this blog. Well, I absolutely doubled the time I was planning to spend. But it was still a very important mindset to have. So here we go!


### Are people giving the EV a five star?
Before I did anything to the raw data, I took a look at the frequency of the star ratings. The website used star ratings so I transformed those stars into discrete values from 1 to 5. Thanks to the magic of `ggplot2`, I generated a histogram while distinguishing those human typed comments from the automatic comments. Unfortunately, there are only 71 ratings that contain informative comments for further analysis. Among all ratings, people seemed to give three-star rating more often. To dig deeper in those reviews, I needed to look at how people actually commented about their experience. Sometimes people give a low rating despite all the wonderful things they mention in the comments. And I wouldn't want to miss that.

![Image of ggplot](/images/posts/evscrape/ggplot.png)

```r
with(dataset, hist(rating)) #Code for the raw figure on the top left

#Code for the pretty one on the top right!
ggplot(dataset_types, aes(x=rating, fill = types)) +
  geom_histogram(binwidth = 1, alpha=0.9, position="identity")+
  xlab("Star Rating (1-5)")+
  ylab("Count of Ratings")+
  labs(title = "Rating Counts (Types of Comments)")+
  scale_fill_discrete(name="Comment Types")+
  theme(text = element_text(size=16),
        axis.text.x = element_text(size=12),
        axis.text.y = element_text(size=12))
```

### WARNING: Here comes more technical stuff!
After gathering the raw data, I was ready to try the `sentimentr` package in R. With the help of this R package, each review is broken into sentences. Then every word in those sentences are looked up in a dictionary of polarized words and tagged based on its level of positivity (+1) and negativity (-1). I will skip some of the other processes involved (e.g. valence shifters). Ultimately, the package generates a sentiment score for each review, indicating its aggregated sentiments from all words. I was able to follow this [article](https://justrthings.com/2016/08/17/web-scraping-and-sentiment-analysis-of-amazon-reviews/comment-page-1/#comment-363) for the most part.

### Did people's comments match up with their ratings?
Holding on to my 71 real comments, I moved on to do sentiment analysis. For each review, I generated a score that indicates the level of positivity or negativity. I then aggregated those reviews by month and plotted the monthly averaged sentiments over time. To compare it with the star rating, I plotted the monthly averaged value along side with it.


![Image of rating](/images/posts/evscrape/rating.png)

Interestingly, the sentiments have obviously become more positive over the time while the ratings seemed to stick around 3. Keep in mind that the left and right y-axis are the scales for ratings and sentiments, respectively. This figure suggests that even though the ratings had no strong evidence of increasingly positive reviews, people had been using more positive words in their recent comments.

To verify the effectiveness of the method, I pulled up the best reviews identified by the `sentimentr` package. Green indicates that those sentences are considered positive. As you can see from the example below , one of the best reviews was actually mischaracterized. The customer was trying to complain about short driving range but the word "please" probably misled the computer. This implies an essential issue in sentiment analysis: *we need to develop specific "dictionaries" for different contexts*. We need the computer to know that "long range" is a positive thing in terms of driver's experience. Regardless, using `sentimentr` remains to be a quick and easy method to grasp a sense of the reviewersâ€™ opinions. 

![image of bestreview](/images/posts/evscrape/bestreview.png)

### How do we create a "dictionary" for EV comments?

The answer is: apply machine learning! In this case, we need to use supervised learning to process the reviews and generate the implied sentiments. It simply means that we will show the model what is a positive or negative review first before letting it classify more auto reviews. I am still fairly new to the machine learning field so I reached out to my friends who are enrolled in a deep learning class. One of their assignments was to use word embeddings in deep models like [Simple Word Embedding Model](http://people.ee.duke.edu/~lcarin/acl2018_swem.pdf) which I found very useful and straightforward for this sentiment analysis exercise. 

<div class="notice">
    <h4>Simple Word Embedding Model</h4>
    <p>This model maps words as vectors and group ones with similar meanings, forming a "dictionary". Using the mapped information, a neural network is then trained to classify the reviews as positive or negative.</p>
</div>



![negative](/images/posts/evscrape/negative.png)

All the Python codes that I used were borrowed from the class material. All I had to do was to adjust some of the values to fit my data. First, I had to manually break down the reviews and give them positive and negative ratings (100 in total), as shown above. "0" stands for negative reviews while "1" stands for positive reviews. I could also apply Recurrent Neural Networks to directly run the undivided long reviews but I tried the simpler method for the sake of time and clarity.

Before training the model, I split 3/4 of the reviews for training and the rest for testing. I then ran through the dataset with my model 250 times to maximize the accuracy, which ended up to be 0.64. This level of accuracy was expected due to my limited review data. In order to examine what the model has learned, I tested words like "range" and "blinker" in the constructed "dictionary". While those words normally have neutral meanings and thus sentiment values around 0.5, they both had a sentiment score around 0.3 in this particular model, towards the negative end (0). This is because they were mostly used in negative reviews. The results successfully reflected the complaints on the low driving range and difficulty to turn the blinkers off. My model worked (somewhat decently)!


```r
#Training the model
sess = tf.InteractiveSession()
sess.run(initialize_all)
for epoch in range(250):
    for batch in range(train_batches):
        data = train[batch*batch_size:(batch+1)*batch_size]
        reviews = [sample['x'] for sample in data]
        labels  = [sample['y'] for sample in data]
        labels = np.array(labels).reshape([-1, 1])
        _, l, acc = sess.run([train_step, loss, accuracy], feed_dict={X: reviews, y: labels})
    if epoch % 10 == 0:
        print("Epoch", epoch, "Loss", l, "Acc", acc)
    random.shuffle(train)

# Evaluate on test set
test_reviews = [sample['x'] for sample in test]
test_labels  = [sample['y'] for sample in test]
test_labels = np.array(test_labels).reshape([-1, 1])
acc = sess.run(accuracy, feed_dict={X: test_reviews, y: test_labels})
print("Final accuracy:", acc)

```

### Phew...
I definitely learned more about data scrapping and content analysis through this project. Built on this work, aspect based sentiment analysis will be a next step to examine what aspects and features of the EVs are people most concerned or excited about. I am a little standing on my toes for the machine learning part but I can't wait to practice it more in the data science project on medical data next spring. Hopefully this article wasn't too overwhelming to read. Overall, I am glad to see people leaving more positive reviews on the EVs over time. I do hope to own one someday!

